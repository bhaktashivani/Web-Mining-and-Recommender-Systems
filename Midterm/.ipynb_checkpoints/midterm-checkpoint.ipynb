{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fadc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e39c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will suppress any warnings, comment out if you'd like to preserve them\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcdcf1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check formatting of submissions\n",
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a8d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84568759",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"spoilers.json.gz\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b15a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for l in f:\n",
    "    d = eval(l)\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "043724ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddaa6c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ae22c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '3037e45de740da82703e55a19f94cfbe',\n",
       " 'timestamp': '2012-11-21',\n",
       " 'review_sentences': [[0,\n",
       "   \"Wolfsbane started off slow... but it was OK since it's about their history and getting the answers I've been questioning in Nightshade.\"],\n",
       "  [0,\n",
       "   'After digesting the information, Calla and the Searches are now setting the motion .'],\n",
       "  [0, 'Plot line is more fast paced and action pack.'],\n",
       "  [0, 'Shay is .... possessively annoying'],\n",
       "  [0, 'No no NO .... I want Ren!!'],\n",
       "  [0, 'Not gonna end well.... poor Ren....'],\n",
       "  [0, 'Connor is flirtatiously humorous xD'],\n",
       "  [0, 'Other new characters fit in the story well'],\n",
       "  [0, 'Ending is such a cliffhanger :/']],\n",
       " 'rating': 4,\n",
       " 'has_spoiler': False,\n",
       " 'book_id': '7263429',\n",
       " 'review_id': 'c2d99ab8f9a95016981a302f763dc29f'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23147241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few utility data structures\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "# ratingDict = {} # To retrieve a rating for a specific user/item pair\n",
    "\n",
    "for d in dataset:\n",
    "    u,i = d['user_id'],d['book_id']\n",
    "    reviewsPerUser[u].append(d)\n",
    "    reviewsPerItem[i].append(d)\n",
    "#     ratingDict[(u,i)] = d['rating'] \n",
    "    \n",
    "    \n",
    "# Sort reviews per user by timestamp\n",
    "for u in reviewsPerUser:\n",
    "    reviewsPerUser[u].sort(key=lambda x: x['timestamp'])\n",
    "    \n",
    "# Same for reviews per item\n",
    "for i in reviewsPerItem:\n",
    "    reviewsPerItem[i].sort(key=lambda x: x['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "742587d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2012-03-13',\n",
       " '2013-05-06',\n",
       " '2013-09-03',\n",
       " '2015-04-05',\n",
       " '2016-02-10',\n",
       " '2016-05-29']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E.g. reviews for this user are sorted from earliest to most recent\n",
    "[d['timestamp'] for d in reviewsPerUser['b0d7e561ca59e313b728dc30a5b1862e']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b08642",
   "metadata": {},
   "source": [
    "## 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1386ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "140dfbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3696"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove any user who has only 1 or less reviews\n",
    "user_1 = [] \n",
    "for key, value in reviewsPerUser.items(): \n",
    "    if len(value) <= 1: \n",
    "        user_1.append(key)\n",
    "[reviewsPerUser.pop(u) for u in user_1]\n",
    "# len(reviewsPerUser)\n",
    "\n",
    "# remove any items who has only 1 or less reviews\n",
    "items_1 = [] \n",
    "for key, value in reviewsPerItem.items(): \n",
    "    if len(value) <= 1: \n",
    "        items_1.append(key)\n",
    "[reviewsPerItem.pop(i) for i in items_1]\n",
    "len(reviewsPerItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73c9d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the averages \n",
    "userAverages = {}\n",
    "itemAverages = {}\n",
    "\n",
    "y = []\n",
    "for u in reviewsPerUser:\n",
    "    rs = [i['rating'] for i in reviewsPerUser[u]]\n",
    "    y.append(rs[-1])\n",
    "    userAverages[u] = sum(rs[:-1]) / (len(rs)-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb364612",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = [userAverages[u] for u in userAverages]\n",
    "# len(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "373cff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1a'] = MSE(y,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcd5ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q1a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843a08a1",
   "metadata": {},
   "source": [
    "## 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c38c9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in reviewsPerItem:\n",
    "    rs = [j['rating'] for j in reviewsPerItem[i]]\n",
    "    y.append(rs[-1])\n",
    "    itemAverages[i] = sum(rs[:-1]) / (len(rs)-1)\n",
    "ypred = [itemAverages[u] for u in itemAverages]\n",
    "# len(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cccbe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1b'] = MSE(y,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7288fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q1b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2972de",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0abf5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAvg(reviews,N): \n",
    "    # Get the averages \n",
    "    Averages = {}\n",
    "    \n",
    "    y = []\n",
    "    for u in reviews:\n",
    "        rs = [i['rating'] for i in reviews[u]]\n",
    "        y.append(rs[-1])\n",
    "        if len(rs) < N+1:\n",
    "    \n",
    "            Averages[u] = sum(rs[:-1]) / (len(rs)-1)    \n",
    "        else:\n",
    "            lastN = rs[-(N+1):][:-1]\n",
    "            Averages[u] = sum(lastN) / (len(lastN))  \n",
    "            \n",
    "    return y,Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcd540f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = []\n",
    "\n",
    "for N in [1,2,3]:\n",
    "    y, userAverages = computeAvg(reviewsPerUser, N)\n",
    "    ypred = [userAverages[u] for u in userAverages]\n",
    "    answers['Q2'].append(MSE(y,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1b4ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q2'], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab8469",
   "metadata": {},
   "source": [
    "## 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ccbb8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort reviews per user by timestamp\n",
    "for u in reviewsPerUser:\n",
    "    reviewsPerUser[u].sort(key=lambda x: x['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ddd5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature3(N, u): # For a user u and a window size of N\n",
    "    rs = [i['rating'] for i in reviewsPerUser[u]]\n",
    "#     print(rs)\n",
    "#     lastN = rs[-(N+1):][:-1]\n",
    "    lastN = rs[-(N+1):-1]\n",
    "    lastN.reverse()\n",
    "    feat = [1] + lastN\n",
    "    \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05e622a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3a'] = [feature3(2,dataset[0]['user_id']), feature3(3,dataset[0]['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f839c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q3a']) == 2\n",
    "assert len(answers['Q3a'][0]) == 3\n",
    "assert len(answers['Q3a'][1]) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f6dee",
   "metadata": {},
   "source": [
    "## 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4146d926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5608319121482543, 1.5409512373315701, 1.5396484853948416]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "answers['Q3b'] = []\n",
    "\n",
    "for N in [1,2,3]:\n",
    "    X = []\n",
    "    Y = []\n",
    "    for u in reviewsPerUser: \n",
    "        rs = [ i['rating'] for i in reviewsPerUser[u] ]\n",
    "        x = feature3(N, u)\n",
    "        if len(x) < N+1: \n",
    "            continue \n",
    "        X.append(x)\n",
    "        Y.append(rs[-1])\n",
    "        \n",
    "    theta,residuals,rank,s = np.linalg.lstsq(X, Y)\n",
    "    mse = ((Y - np.dot(X, theta))**2).mean()\n",
    "    answers['Q3b'].append(mse)\n",
    "# answers['Q3b'].append(0)\n",
    "# answers['Q3b'].append(0)\n",
    "answers['Q3b']\n",
    "# print(len(X), len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d512b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q3b'], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c699d0",
   "metadata": {},
   "source": [
    "## 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4aab34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "globalAverage = [d['rating'] for d in dataset]\n",
    "globalAverage = sum(globalAverage) / len(globalAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2676be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMeanValue(N, u): # For a user u and a window size of N\n",
    "    \n",
    "    rs = [i['rating'] for i in reviewsPerUser[u]]\n",
    "    if len(rs) < N+1:\n",
    "        Averages = sum(rs[:-1]) / (len(rs)-1) \n",
    "        x = [Averages]* (N+1 - len(rs))\n",
    "        X = feature3(N, u) + x\n",
    "    else:\n",
    "        X = feature3(N, u) \n",
    "\n",
    "    return X\n",
    "\n",
    "# featureMeanValue(10, dataset[0]['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "270cf89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMissingValue(N, u):\n",
    "    \n",
    "    rs = [i['rating'] for i in reviewsPerUser[u]]\n",
    "\n",
    "    f4 = [1]\n",
    "    for f in range(1,len(feature3(N, u))):\n",
    "        f4.append(0) \n",
    "        f4.append(feature3(N, u)[f]) \n",
    "\n",
    "    if len(rs) < N+1:\n",
    "        x = [1,0]* (N+1 - len(rs))\n",
    "        X = f4 + x\n",
    "    else:\n",
    "        X= f4  \n",
    "            \n",
    "    return X\n",
    "\n",
    "# featureMissingValue(10, dataset[0]['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58791bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4a'] = [featureMeanValue(10, dataset[0]['user_id']), featureMissingValue(10, dataset[0]['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8f12e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers['Q4a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3c28e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q4a']) == 2\n",
    "assert len(answers['Q4a'][0]) == 11\n",
    "assert len(answers['Q4a'][1]) == 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90df2e9",
   "metadata": {},
   "source": [
    "## 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59a7c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4b'] = []\n",
    "X = []\n",
    "Y = []\n",
    "N = 10\n",
    "for u in reviewsPerUser: \n",
    "    rs = [ i['rating'] for i in reviewsPerUser[u] ]\n",
    "    X.append(featureMeanValue(N, u))\n",
    "    Y.append(rs[-1])\n",
    "\n",
    "theta,residuals,rank,s = np.linalg.lstsq(X, Y)\n",
    "mse = ((Y - np.dot(X, theta))**2).mean()\n",
    "answers['Q4b'].append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14318af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "N = 10\n",
    "for u in reviewsPerUser: \n",
    "    rs = [ i['rating'] for i in reviewsPerUser[u] ]\n",
    "    X.append(featureMissingValue(N, u))\n",
    "    Y.append(rs[-1])\n",
    "\n",
    "theta,residuals,rank,s = np.linalg.lstsq(X, Y)\n",
    "mse = ((Y - np.dot(X, theta))**2).mean()\n",
    "answers['Q4b'].append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73fabbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers['Q4b'] = []\n",
    "\n",
    "# for featFunc in [featureMeanValue, featureMissingValue]:\n",
    "#     # etc.\n",
    "    \n",
    "#     answers['Q4b'].append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e348489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers[\"Q4b\"], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d114396",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cee7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature5(sentence):\n",
    "    f3 = [1 for c in sentence if c.isupper()]\n",
    "    feat = [1,len(sentence), sentence.count('!'), sum(f3)]\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "580ed238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BER_(predictions, y):\n",
    "    # Balance Error Rate function from HW2 \n",
    "    TP = sum([(p and l) for (p,l) in zip(predictions, y)])\n",
    "    FP = sum([(p and not l) for (p,l) in zip(predictions, y)])\n",
    "    TN = sum([(not p and not l) for (p,l) in zip(predictions, y)])\n",
    "    FN = sum([(not p and l) for (p,l) in zip(predictions, y)])\n",
    "    \n",
    "    TPR = TP / (TP + FN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    BER = 1 - 0.5*(TPR + TNR) #balanced Error Rate \n",
    "    return TP, TN, FP, FN, BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "426ca2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "X = []\n",
    "\n",
    "for d in dataset:\n",
    "    for spoiler,sentence in d['review_sentences']:\n",
    "        X.append(feature5(sentence))\n",
    "        y.append(spoiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0dfd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LogisticRegression(C=1.0, class_weight=\"balanced\")\n",
    "mod.fit(X, y)\n",
    "\n",
    "predictions = mod.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "299e52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, TN, FP, FN, BER = BER_(predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a94d7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5a'] = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "116c5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5b'] = [TP, TN, FP, FN, BER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0c96525",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q5a']) == 4\n",
    "assertFloatList(answers['Q5b'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a77a3",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "193e94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature6(review):\n",
    "    X = [1]\n",
    "    N = 5\n",
    "    for N in range(len(review)):\n",
    "        if N < 5: \n",
    "            spoiler = review[N][0]\n",
    "            X.append(spoiler)\n",
    "        elif N == 5: \n",
    "            X = X + feature5(review[N][1])[1:]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a437dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "X = []\n",
    "\n",
    "for d in dataset:\n",
    "    sentences = d['review_sentences']\n",
    "    if len(sentences) < 6: continue\n",
    "    X.append(feature6(sentences))\n",
    "    y.append(sentences[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92aeec32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 75, 0, 1]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c61a5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6a'] = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d4c0206",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LogisticRegression(C=1.0, class_weight=\"balanced\")\n",
    "mod.fit(X, y)\n",
    "\n",
    "predictions = mod.predict(X)\n",
    "\n",
    "TP, TN, FP, FN, BER = BER_(predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f977c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6b'] = BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0be28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q6a']) == 9\n",
    "assertFloat(answers['Q6b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5781f5",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c01c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50/25/25% train/valid/test split\n",
    "Xtrain, Xvalid, Xtest = X[:len(X)//2], X[len(X)//2:(3*len(X))//4], X[(3*len(X))//4:]\n",
    "ytrain, yvalid, ytest = y[:len(X)//2], y[len(X)//2:(3*len(X))//4], y[(3*len(X))//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f433ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Got most of this code from Chapter 3 workbook and edited it as needed. \n",
    "\n",
    "def pipeline(reg):\n",
    "    mod = linear_model.LogisticRegression(C=reg, class_weight='balanced')\n",
    "        \n",
    "    mod.fit(Xtrain,ytrain)\n",
    "    ypredValid = mod.predict(Xvalid)\n",
    "    ypredTest = mod.predict(Xtest)\n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    TP, TN, FP, FN, berVal = BER_(ypredValid,yvalid)\n",
    "    print(\"C = \" + str(reg) + \"; validation BER = \" + str(berVal))\n",
    "    \n",
    "    # test\n",
    "\n",
    "    TP, TN, FP, FN, berTest = BER_(ypredTest,ytest)\n",
    "#     print(\"C = \" + str(reg) + \"; test BER = \" + str(berTest))\n",
    "\n",
    "    return mod, berVal, berTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c253fc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.01; validation BER = 0.13345081097468547\n",
      "C = 0.1; validation BER = 0.13276868273457632\n",
      "C = 1; validation BER = 0.14319766560557823\n",
      "C = 10; validation BER = 0.14268606942549644\n",
      "C = 100; validation BER = 0.1423450053054418\n"
     ]
    }
   ],
   "source": [
    "modList = []\n",
    "berList = []\n",
    "berTestList = []\n",
    "\n",
    "for c in [0.01, 0.1, 1, 10, 100]:\n",
    "    # etc.\n",
    "    mod, berVal, berTest = pipeline(c)\n",
    "    modList.append(mod)\n",
    "    berList.append(berVal) \n",
    "    berTestList.append(berTest)\n",
    "bers = berList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd45066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since C=0.1 has the smallest validation error we choose that as a general good performer \n",
    "bestC = 0.1\n",
    "ber = berTestList[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8389608",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = bers + [bestC] + [ber]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d53b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q7'], 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219952da",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38a6c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57f30ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 5000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 75/25% train/test split\n",
    "dataTrain = dataset[:15000]\n",
    "dataTest = dataset[15000:]\n",
    "len(dataTrain),len(dataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1d770bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few utilities\n",
    "\n",
    "userAverages = defaultdict(list)\n",
    "itemAverages = defaultdict(list)\n",
    "ratingMean = []\n",
    "\n",
    "for d in dataTrain:\n",
    "    \n",
    "    userAverages[d['user_id']].append(d['rating'])\n",
    "    itemAverages[d['book_id']].append(d['rating'])\n",
    "    ratingMean.append(d['rating'])\n",
    "\n",
    "# for i,u in zip(itemAverages, userAverages):\n",
    "#     itemAverages[i] = sum(itemAverages[i]) / len(itemAverages[i])\n",
    "#     userAverages[u] = sum(userAverages[u]) / len(userAverages[u])\n",
    "for i in itemAverages:\n",
    "    itemAverages[i] = sum(itemAverages[i]) / len(itemAverages[i])\n",
    "    \n",
    "for u in userAverages:\n",
    "    userAverages[u] = sum(userAverages[u]) / len(userAverages[u])\n",
    "\n",
    "ratingMean = sum(ratingMean) / len(ratingMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62952595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviewsPerItem = defaultdict(list)\n",
    "itemsPerUser = defaultdict(set) \n",
    "\n",
    "reviewsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(set)\n",
    "\n",
    "ratingDict = {} # To retrieve a rating for a specific user/item pair\n",
    "\n",
    "for d in dataTrain:\n",
    "    u,i = d['user_id'], d['book_id'] \n",
    "    \n",
    "    usersPerItem[i].add(u)\n",
    "    itemsPerUser[u].add(i)\n",
    "    \n",
    "    reviewsPerUser[u].append(d)\n",
    "#     reviewsPerItem[i].append(d)\n",
    "    ratingDict[(u,i)] = d['rating'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e0ab533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRating(user,item):\n",
    "    \n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    \n",
    "    for i2 in usersPerItem[item]:\n",
    "        \n",
    "        if i2 == user: continue\n",
    "        \n",
    "        ratings.append(ratingDict[(i2,item)] - userAverages[i2])\n",
    "        similarities.append(Jaccard(itemsPerUser[user],itemsPerUser[i2]))\n",
    "        \n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "    \n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        if item in itemAverages:\n",
    "            return itemAverages[item]\n",
    "        else:\n",
    "            return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db0e1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [predictRating(d['user_id'], d['book_id']) for d in dataTest]\n",
    "labels = [d['rating'] for d in dataTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a72e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4891766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8218848124540317"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[\"Q8\"] = MSE(predictions, labels)\n",
    "answers[\"Q8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "789b53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers[\"Q8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296ca51",
   "metadata": {},
   "source": [
    "## 9a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f498d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "\n",
    "for d in dataTrain:\n",
    "    u,i = d['user_id'], d['book_id'] \n",
    "    usersPerItem[i].add(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5930c59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.742012484444442"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE for instances where i never appeared in the training set\n",
    "itemsTrain = set(usersPerItem.keys())\n",
    "# print(itemsTrain)\n",
    "itemsTest_unique = []\n",
    "\n",
    "uniqueItemsRating = []\n",
    "\n",
    "for d in dataTest:\n",
    "#     print(d['book_id'])\n",
    "    if d['book_id'] not in itemsTrain: \n",
    "        itemsTest_unique.append(d)\n",
    "#         print(itemsTest_unique)\n",
    "        uniqueItemsRating.append(d['rating'])\n",
    "predictions = [predictRating(d['user_id'], d['book_id']) for d in itemsTest_unique]\n",
    "labels = [r for r in uniqueItemsRating]\n",
    "mse0 = MSE(predictions, labels)  \n",
    "mse0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3064cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE for instances where i appeared in the training set for atleast once but no more than 5 times. \n",
    "itemsTrain1to5 = set()\n",
    "itemsTrain6plus = set()\n",
    "\n",
    "for i in usersPerItem: \n",
    "    num_reviewed = len(usersPerItem[i])\n",
    "    if num_reviewed > 5: \n",
    "        itemsTrain6plus.add(i)  \n",
    "    else: \n",
    "        itemsTrain1to5.add(i)\n",
    "       \n",
    "    \n",
    "itemsTest1to5 = []\n",
    "itemsTestRating1to5 = []\n",
    "\n",
    "\n",
    "for d in dataTest:\n",
    "    if d['book_id'] in itemsTrain1to5:\n",
    "        itemsTest1to5.append(d)\n",
    "        itemsTestRating1to5.append(d['rating'])       \n",
    "        \n",
    "predictions = [predictRating(d['user_id'], d['book_id']) for d in itemsTest1to5]\n",
    "labels = [r for r in itemsTestRating1to5 ]\n",
    "mse1to5 = MSE(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7f0f34a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_in_test_more_5_train = []\n",
    "r_items_in_test_more_5_train = []\n",
    "\n",
    "for d in dataTest:\n",
    "    if d['book_id'] in itemsTrain6plus:\n",
    "        items_in_test_more_5_train.append(d)\n",
    "        r_items_in_test_more_5_train.append(d['rating'])\n",
    "        \n",
    "predictions = [predictRating(d['user_id'], d['book_id']) for d in items_in_test_more_5_train]\n",
    "labels = [r for r in r_items_in_test_more_5_train]\n",
    "mse5 = MSE(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b8b8ce68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0366290574931427"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse1to5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1d2a4664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4334148860349525"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d269238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[\"Q9\"] = [mse0, mse1to5, mse5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ebfff50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers[\"Q9\"], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e12f17",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a0528a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "\n",
    "for d in dataTrain:\n",
    "    u,i = d['user_id'], d['book_id'] \n",
    "    usersPerItem[i].add(u)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e2fea856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewpredictRating(user,item):\n",
    "    \n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    \n",
    "    for d in reviewsPerUser[item]:\n",
    "        i2 = d['book_id']\n",
    "        if i2 == user: continue\n",
    "#         ratingDict[(i2,item)]\n",
    "        ratings.append(ratingDict[(i2,item)] - itemAverages[i2])\n",
    "        \n",
    "        similarities.append(Jaccard(usersPerItem[item],usersPerItem[i2]))\n",
    "        \n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "    \n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "            \n",
    "        if item in itemAverages:\n",
    "            return itemAverages[item]\n",
    "        elif user in userAverages:\n",
    "            return userAverages[user]\n",
    "        else:\n",
    "            return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c10c5b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6696633366192306"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE for instances where i never appeared in the training set\n",
    "itemsTrain = set(usersPerItem.keys())\n",
    "# print(itemsTrain)\n",
    "itemsTest_unique = []\n",
    "\n",
    "uniqueItemsRating = []\n",
    "\n",
    "for d in dataTest:\n",
    "#     print(d['book_id'])\n",
    "    if d['book_id'] not in itemsTrain: \n",
    "        itemsTest_unique.append(d)\n",
    "        uniqueItemsRating.append(d['rating'])\n",
    "predictions = [NewpredictRating(d['user_id'], d['book_id']) for d in itemsTest_unique]\n",
    "labels = [r for r in uniqueItemsRating]\n",
    "itsMSE = MSE(predictions, labels)  \n",
    "itsMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bf21aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solution = \"If we don't have the item in the training set, we can take the average rating for the given user for other items and return that as a prediction. Therefore, instead of prediction mean of all item, we are getting a geenral idea of how a specific user tends to rate their products. This slightly performs better as there is some correlation there, however it is not much of a difference as expected. It is still in the range from part a to part c MSE from previous part\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "305d3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[\"Q10\"] = (Solution, itsMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d0613500",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(answers[\"Q10\"][0]) == str\n",
    "assertFloat(answers[\"Q10\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "436d2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_midterm.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6da8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
