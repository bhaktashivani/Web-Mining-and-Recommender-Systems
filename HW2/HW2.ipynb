{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55e9abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSubmitted by: Shivani Bhakta \\nThis is CSE 258 Homework assignment. \\nCredit: \\nSome of the code was given as stub and some was inspired/copied from the lecture notes by McAuley, Julian and his book.  \\n@book{mcauley2022,\\n      title     = \"Personalized Machine Learning\",\\n      author    = \"McAuley, Julian\",\\n      year      = \"in press\",\\n      publisher = \"Cambridge University Press\"\\n    }\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## CSE 158/258, Fall 2022: Homework 1\n",
    "'''\n",
    "Submitted by: Shivani Bhakta \n",
    "This is CSE 258 Homework assignment. \n",
    "Credit: \n",
    "Some of the code was given as stub and some was inspired/copied from the lecture notes by McAuley, Julian and his book.  \n",
    "@book{mcauley2022,\n",
    "      title     = \"Personalized Machine Learning\",\n",
    "      author    = \"McAuley, Julian\",\n",
    "      year      = \"in press\",\n",
    "      publisher = \"Cambridge University Press\"\n",
    "    }\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbcb905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# from pathlib import Path\n",
    "# import sys\n",
    "# sys.path.append(\"../\")\n",
    "# sys.path.append(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbbc2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c00feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/5year.arff\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fad4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse the data\n",
    "while not '@data' in f.readline():\n",
    "    pass\n",
    "\n",
    "dataset = []\n",
    "for l in f:\n",
    "    if '?' in l: # Missing entry\n",
    "        continue\n",
    "    l = l.split(',')\n",
    "    values = [1] + [float(x) for x in l]\n",
    "    values[-1] = values[-1] > 0 # Convert to bool\n",
    "    dataset.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7691e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [d[:-1] for d in dataset]\n",
    "y = [d[-1] for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03249990",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {} # Your answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a31a5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, y):\n",
    "    acc = (predictions == y)\n",
    "    return sum(acc)/len(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83974166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BER(predictions, y):\n",
    "    \n",
    "    TP = sum([(p and l) for (p,l) in zip(predictions, y)])\n",
    "    FP = sum([(p and not l) for (p,l) in zip(predictions, y)])\n",
    "    TN = sum([(not p and not l) for (p,l) in zip(predictions, y)])\n",
    "    FN = sum([(not p and l) for (p,l) in zip(predictions, y)])\n",
    "    \n",
    "    TPR = TP / (TP + FN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    BER = 1 - 0.5*(TPR + TNR) #balanced Error Rate \n",
    "    return BER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e78a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f59633dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LogisticRegression(C=1)\n",
    "mod.fit(X,y)\n",
    "\n",
    "pred = mod.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf148f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = accuracy(pred, y)\n",
    "ber1 = BER(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "033a6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1'] = [acc1, ber1] # Accuracy and balanced error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e75988a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q1'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30482ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc8f8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LogisticRegression(C=1, class_weight='balanced')\n",
    "mod.fit(X,y)\n",
    "\n",
    "pred = mod.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e99274d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc2 = accuracy(pred, y)\n",
    "ber2 = BER(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de8d6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = [acc2, ber2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a90cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q2'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1fa1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55d4beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3)\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d19c0c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0.01897,\n",
       " 0.57234,\n",
       " -0.041892,\n",
       " 0.8924,\n",
       " -261.31,\n",
       " 0.0,\n",
       " 0.020566,\n",
       " 0.7472,\n",
       " 0.47,\n",
       " 0.42766,\n",
       " 0.044181,\n",
       " 0.052826,\n",
       " 0.077776,\n",
       " 0.020566,\n",
       " 5714.9,\n",
       " 0.063868,\n",
       " 1.7472,\n",
       " 0.020566,\n",
       " 0.043758,\n",
       " 144.42,\n",
       " 1.0607,\n",
       " 0.019188,\n",
       " 0.040361,\n",
       " 0.033148,\n",
       " 0.37757,\n",
       " 0.061079,\n",
       " 0.81253,\n",
       " -0.064196,\n",
       " 5.813,\n",
       " 1.1652,\n",
       " 0.084718,\n",
       " 427.73,\n",
       " 0.89734,\n",
       " 0.61039,\n",
       " 0.13778,\n",
       " 0.47,\n",
       " 0.99499,\n",
       " 0.58993,\n",
       " 0.29315,\n",
       " 0.067762,\n",
       " 0.54236,\n",
       " 0.040826,\n",
       " 249.32,\n",
       " 104.9,\n",
       " 0.10201,\n",
       " 0.41473,\n",
       " 194.3,\n",
       " 0.0032,\n",
       " 0.006808,\n",
       " 0.60702,\n",
       " 0.38932,\n",
       " 1.1144,\n",
       " 0.65533,\n",
       " 0.904,\n",
       " 0.8924,\n",
       " 0.29315,\n",
       " 0.044357,\n",
       " 0.7382,\n",
       " 0.37945,\n",
       " 2.5274,\n",
       " 3.4794,\n",
       " 302.34,\n",
       " 1.2072,\n",
       " 0.72022]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [d[:-1] for d in dataset]\n",
    "y = [d[-1] for d in dataset]\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18d5fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xvalid, Xtest = X[:len(X)//2], X[len(X)//2:(3*len(X))//4], X[(3*len(X))//4:]\n",
    "ytrain, yvalid, ytest = y[:len(X)//2], y[len(X)//2:(3*len(X))//4], y[(3*len(X))//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d66f07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1515, 758, 758)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtrain), len(Xvalid), len(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aad3140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LogisticRegression(C=1, class_weight='balanced')\n",
    "mod.fit(Xtrain,ytrain)\n",
    "\n",
    "ypredtrain = mod.predict(Xtrain)\n",
    "ypredvalid = mod.predict(Xvalid)\n",
    "ypredTest = mod.predict(Xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "647021ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "berTrain = BER(ypredtrain, ytrain)\n",
    "berValid = BER(ypredvalid, yvalid)\n",
    "berTest = BER(ypredTest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bb40dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = [berTrain, berValid, berTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e0ece86",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q3'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81d44cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ff0daca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.0001; validation BER = 0.3281320669380371\n",
      "C = 0.001; validation BER = 0.31931252826775225\n",
      "C = 0.01; validation BER = 0.3281320669380371\n",
      "C = 0.1; validation BER = 0.3179556761646314\n",
      "C = 1; validation BER = 0.3159203980099503\n",
      "C = 10; validation BER = 0.3111714156490276\n",
      "C = 100; validation BER = 0.2955030044582283\n",
      "C = 1000; validation BER = 0.29618143050978873\n",
      "C = 10000; validation BER = 0.29618143050978873\n"
     ]
    }
   ],
   "source": [
    "## Got most of this code from Chapter 3 workbook and edited it as needed. \n",
    "\n",
    "def pipeline(reg):\n",
    "    mod = linear_model.LogisticRegression(C=reg, class_weight='balanced')\n",
    "        \n",
    "    mod.fit(Xtrain,ytrain)\n",
    "    ypredValid = mod.predict(Xvalid)\n",
    "    ypredTest = mod.predict(Xtest)\n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    berVal = BER(ypredValid,yvalid)\n",
    "    print(\"C = \" + str(reg) + \"; validation BER = \" + str(berVal))\n",
    "    \n",
    "    # test\n",
    "\n",
    "    berTest = BER(ypredTest,ytest)\n",
    "#     print(\"C = \" + str(reg) + \"; test BER = \" + str(berTest))\n",
    "\n",
    "    return mod, berVal, berTest\n",
    "\n",
    "modList = []\n",
    "berList = []\n",
    "berTestList = []\n",
    "for c in [1e-04, 1e-03, 1e-02, 1e-01, 1, 10, 100, 1000, 10000]:\n",
    "    mod, berVal, berTest = pipeline(c)\n",
    "    modList.append(mod)\n",
    "    berList.append(berVal) \n",
    "    berTestList.append(berTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c96b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4'] = berList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f55f3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q4'], 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b455b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a80d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since C=100 has the smallest validation error we choose that as a general good performer \n",
    "bestC = 100\n",
    "ber5 = berTestList[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62bdaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = [bestC, ber5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8cafe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q5'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcbc2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ace19c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open(\"young_adult_10000.json.gz\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(eval(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06598b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = dataset[:9000]\n",
    "dataTest = dataset[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09f07e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 1000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataTrain), len(dataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4209458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data structures you might want\n",
    "\n",
    "usersPerItem = defaultdict(set) # Maps an item to the users who rated it\n",
    "itemsPerUser = defaultdict(set) # Maps a user to the items that they rated\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "ratingDict = {} # To retrieve a rating for a specific user/item pair\n",
    "\n",
    "for d in dataTrain:\n",
    "    user, item, reviews = d['user_id'], d['book_id'], d['review_id']\n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)\n",
    "    \n",
    "    \n",
    "    reviewsPerUser[user].append(d)\n",
    "    reviewsPerItem[item].append(d)\n",
    "    ratingDict[(user,item)] = d['rating'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03c90f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    num = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    \n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return num/denom\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25bfacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar(i, N):\n",
    "    similarities = []\n",
    "    users = usersPerItem[i]\n",
    "    \n",
    "    candidateItems = set()\n",
    "    for u in users:\n",
    "        candidateItems = candidateItems.union(itemsPerUser[u])\n",
    "        \n",
    "    for ii in candidateItems: #usersPerItem:\n",
    "        if ii == i: continue\n",
    "        sim = Jaccard(users, usersPerItem[ii])\n",
    "        similarities.append((sim,ii))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:N]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2652a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6'] = mostSimilar('2767052', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35457af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q6']) == 10\n",
    "assertFloatList([x[0] for x in answers['Q6']], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69798ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5fb4cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "userAverages = {}\n",
    "itemAverages = {}\n",
    "\n",
    "for u in itemsPerUser:\n",
    "    rs = [ratingDict[(u,i)] for i in itemsPerUser[u]]\n",
    "    userAverages[u] = sum(rs) / len(rs)\n",
    "    \n",
    "for i in usersPerItem:\n",
    "    rs = [ratingDict[(u,i)] for u in usersPerItem[i]]\n",
    "    itemAverages[i] = sum(rs) / len(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0013fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRating(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['book_id']\n",
    "        \n",
    "        if i2 == item: continue\n",
    "        \n",
    "        ratings.append(d['rating'] - itemAverages[i2])\n",
    "        similarities.append(Jaccard(usersPerItem[item],usersPerItem[i2]))\n",
    "        \n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(similarities)\\\n",
    "    \n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        ratingMean = sum([d['rating'] for d in dataTrain]) / len(dataTrain)\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83dc5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03c18399",
   "metadata": {},
   "outputs": [],
   "source": [
    "simPredictions = [predictRating(d['user_id'], d['book_id']) for d in dataTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec57153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [d['rating'] for d in dataTest]\n",
    "mse7 = MSE(simPredictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e3f9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = mse7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7d294f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "088d0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e5227c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRating(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerItem[item]:\n",
    "        u2 = d['user_id']\n",
    "        \n",
    "        if u2 == user: continue\n",
    "        \n",
    "        ratings.append(d['rating'])\n",
    "        similarities.append(Jaccard(itemsPerUser[item],itemsPerUser[u2]))\n",
    "        \n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return sum(weightedRatings) / sum(similarities)\n",
    "    \n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        ratingMean = sum([d['rating'] for d in dataTrain]) / len(dataTrain)\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f7b51a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "simPredictions = [predictRating(d['user_id'], d['book_id']) for d in dataTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9c27d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [d['rating'] for d in dataTest]\n",
    "mse8 = MSE(simPredictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2461deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q8'] = mse8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "def088ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f534c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw2.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb717703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
